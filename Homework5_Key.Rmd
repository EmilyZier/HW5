
---
title: "PADP8120_Homework5"
author: "Fall 2015"
date: "![Creative Commons Attribution License](images/cc-by.png)"
output:
  html_document:
    highlight: pygments
    theme: cerulean
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
---


# Homework 5

Guidelines: Homeworks should be clear and legible, with answers clearly indicated and work shown. Homeworks will be given a minus, check, or check plus owing to completion and correctness. You are welcome to work with others but please submit your own work. Your homework must be produced in an R Markdown (.rmd) file submitted via github. If you are having trouble accomplishing this, please refer to the [guide](http://spia.uga.edu/faculty_pages/tyler.scott/teaching/PADP8120_Fall2015/Homeworks/submitting_homework.shtml). 


This homework adapts materials from the work of Michael Lynch (http://spia.uga.edu/faculty_pages/mlynch/) and Matthew Salganik (http://www.princeton.edu/~mjs3/)

## Topics

Topics covered in this homework include:

- Matrix regression 
- Interactions and categorical variables
- Transformations
- Maximum Likelihood

## Problems

### Problem 1. 

Just as you did for Homework 4, write a function that emulates the `lm` function in R for a simple (bivariate) regression. *However, this time your function needs to make use of the matrix regression approach we learned in Week 12.* Like the `lm` function, your function should be able to estimate and report to the screen $\beta_k$ coefficients, standard errors for these coefficients, and corresponding t-values and p-values. It should also report the residual standard error. Be sure to show your code. Compare your results to the results of the `lm` function on some data of your choosing to verify that things are working correctly. 

```{r}
sim.x1 = rnorm(100)
sim.x2 = rnorm(100)
sim.y = matrix(rnorm(100),ncol=1)
x.vars = as.matrix(data.frame(intercept = 1,sim.x1,sim.x2))

matrix.lm = function(outcome.matrix,design.matrix)
{
betas = solve(t(design.matrix) %*% design.matrix) %*% t(design.matrix) %*% outcome.matrix
betas = round(betas,3)
# estimate of sigma-squared
dSigmaSq <- sum((outcome.matrix - design.matrix%*%betas)^2)/(nrow(design.matrix)-ncol(design.matrix))
# variance covariance matrix
VarCovar <- dSigmaSq*chol2inv(chol(t(design.matrix)%*%design.matrix)) 
# coeff. est. standard errors  
vStdErr <- round(sqrt(diag(VarCovar)),3)
df = nrow(outcome.matrix) - length(betas)
t.obs = round(betas/vStdErr,3)
p.vals = round(2 * (1-pt(abs(t.obs),df=df)),3)
return(data.frame(coefs = betas,SE = vStdErr,
                  t.obs = t.obs,p.vals=p.vals))}

matrix.lm(sim.y,x.vars)

summary(lm(sim.y~sim.x1+sim.x2))
```



### Problem 2.

#### Occupational prestige 

Let's try to understand the relationship between typical education, income, job type, and occupation prestigue using the data from Duncan.  You can read the documentation [here](http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-2E/datasets/Duncan.pdf)

Here's some code to read in and clean the data.  And for the purposes of this assignment we are going to exclude professionals.  In other words, we are only concerned about white collar and blue collar occupations.  Again, notice that the unit here is occupations.

```{r message=FALSE,warnings=FALSE}
library(dplyr)
occup <- read.table("input/Duncan.txt", header=TRUE)
occup$state <- rownames(occup)
rownames(occup) <- NULL
occup <- filter(occup, type %in% c("wc", "bc"))
head(occup)
```

(@) Run a regression model to predict the prestige of an occupation based on the level of education of people in that occupation (measured by the percentage of people in the field who have graduated from high school in 1950).

```{r}
summary(mod1 <- lm(prestige~education,occup))
```


(@) Make a plot showing the data and the model that you fit.

```{r}
plot(prestige~education,data=occup)
abline(reg = mod1)
```


(@) Now run a regression model to predict the prestige of an occupation based on the level of education of people in that occupation (measured by the percentage of people in the field who have graduated from high school in 1950) and the occupation type (blue collar/white collar).

```{r}
summary(mod2 <- lm(prestige~education+type,data=occup))
```


(@) Make a plot showing the data and the model that you fit.

```{r}
plot(prestige~education,data=occup)
abline(a=mod2$coef[1],b=mod2$coef[2],col='blue',lwd=3)
abline(a=mod2$coef[1]+mod2$coef[3],b=mod2$coef[2],col='grey80',lwd=3)
```

(@) Now run a regression model to predict occupational prestige based on the level of education and occupation type where the relationship between education and occupational prestige is allowed to vary by occupation type.

```{r}
summary(mod3 <- lm(prestige~education*type,data=occup))
```

(@) Calculate predicted levels of prestige for white collar and blue collar jobs at various levels of income and report these predicted levels in a graph (no table needed). What have you learned about prestige thanks to the interactive variable?

```{r}
pred.vals = predict(mod3,newdata = data.frame(
  education=rep(seq(0,100,10),2),type=rep(c('wc','bc'),each=11)))

plot(x=rep(seq(0,100,10),2),y=pred.vals,col=rep(c('grey80','blue'),each=11),pch=19,xlab='Education',ylab='Predicte Prestige')
```

(@) How would you summarize the conclusions from three models above?

For both white collar and blue collar jobs, the level of education a given profession requires appears to be positively correlated with occupational prestige. However, the prestige of white collar jobs appears to be less dependent upon the level of education required, as each additional percentage point of the proportion of workers with a degree in a blue collar job predicts a 1.01 unit increase in occupational prestige (see Model 3), whereas a one percentage point increase in proportion of workers with a degree in a white collar occupation predicts only a 0.38 unit increase in occupational prestige. It is important to note, however, that neither model 2 (separate intercepts for white and blue collar jobs) or model 3 (separate intercepts and slopes for white and blue collar jobs) fit the data better than model 1 (prestige solely as a function of education).

```{r}
library(car)
anova(mod1,mod2,mod3)
```

Thus, the overall conclusion is that net of the level of education a job requires, the type of job makes little difference for occupational prestige (which is not unexpected since level of education should closely correlate with job-type, such that white/blue collar should be somewhat redundant). Simply comparing the mean education level for white and blue collar jobs in these data make this very apparent:

```{r}
tapply(occup$education,occup$type,mean)
```


(@) Now run a the following regression model: `lm(prestige ∼ income + education + income ∗ education)` and substantively describe the effects of the independent variables on the dependent variable. In other words, describe the relationships implied by the interactive terms. Does this interaction make sense to you? Why or why not? No table needed.

```{r}
summary(mod4 <- lm(prestige ~income+ education + income * education,occup))
```

Model 4 regresses occupational prestige on income, education, and the interactive of income and education. The predicted impact of a one unit change in income on occupational prestige is 0.74 + education * -0.003; in other words, the assocation between income and prestige is conditional on education, such that as education increases the marginal impact of income declines. The same holds true for the predicted impact of education (0.26 + income * -0.003); as income increases, the predicted impact of education remains positive but declines slightly (it is unlikely that income will ever be at a high enough level such that the marginal effect for education would be negative). Substantively, the takeaway here is that for predicting occupational prestige, income and education are to some extent substitutes for one another. That is, a high prestige job could have high income, high education, or some combination of both. 

It is important to note, however, that the interaction term seems to really be inconsequential here; if we compare the restricted model without the interaction for education and income to model 4, the f-test indicates that the restricted model is a better model (since we fail to reject the null that the interaction term does not improve model fit, p > 0.05).

```{r}
testmod = lm(prestige~income+education,occup)
anova(testmod,mod4)
```

(@) Use calculus to identify the predicted impact of a one unit change in income on occupational prestige. Assess whether this impact is statistically distinct from zero. Remember that the variance for an estimated marginal effect $\frac{\partial \hat{y}}{\partial x} = \hat{\beta}_x + \hat{\beta}_{xz}*z$ where x and z are interacted independent variables, can be calculated by:

$$ V(\frac{\partial \hat{y}}{\partial x}) = V(\hat{\beta}_x) + z^2 V(\hat{\beta}_{xz}) + 2z * Cov(\hat{\beta}_x,\hat{\beta}_{xz}) $$

```{r}
#compute marginal effect for different educ levels (educ = 1 to 100)
educ.sim = seq(1:100)
bx = mod4$coef[2]
bxz = mod4$coef[4]
inc.slopes = bx + bxz * educ.sim

#compute variance for interaction term
var.bx = vcov(mod4)[2,2]
var.bxz = vcov(mod4)[4,4]
cov.bx.bxz = vcov(mod4)[4,2]
var.dy.dx = var.bx + educ.sim^2 * var.bxz + 2*educ.sim*cov.bx.bxz

p.vals = 2 * (1-pt(abs(inc.slopes/sqrt(var.dy.dx)),df=nrow(occup)-length(mod4)))
plot(p.vals~educ.sim,ylim=c(0,1),ylab='p-value',xlab='Education')
abline(h=0.05,col='red',lty=2)
```

The code above computes the p-value for the marginal effect of income at education levels between 0 and 100 (i.e., 1 p-value for each simulated education level). As the plot above, shows, the marginal impact of income is significant at the $\alpha = 0.05$ level when educaiton is below 60, and insignificant above. 

(@) ￼Because the marginal effect of x depends on values of z, you will need to assess whether the marginal effect is significant across a range of values of z.

```{r}
upper <- inc.slopes + 1.96*sqrt(var.dy.dx)
lower <- inc.slopes - 1.96*sqrt(var.dy.dx)
plot(educ.sim, inc.slopes , type = "l", lty = 1, xlab = "Level of Education", ylab = "Marginal Effect of Income",ylim=c(-1,1))
points(educ.sim, upper, type = "l", lty = 2)
points(educ.sim, lower, type = "l", lty = 2)
points(educ.sim, rep(0, length(educ.sim)), type = "l", col = "gray")
```

The plot above supports the prior finding; we can observe that the 95% confidence interval for the marginal effect of income does not include 0 for levels of education below 60. 

### Problem 3.

#### LA Housing Prices

Load the LA housing prices dataset:

```{r message = FALSE,eval=TRUE}
la.dat = read.csv('Input/LA.csv')
```

(@) Fit the best model you can to predict housing prices in LA on the basis of theory (i.e., what should matter for house prices?) and model fit (i.e., DO NOT use stepwise regression, but feel free to add/subtract/transform variables as you feel are necessary).

My basic theoretical expectation is that the price of a house will be contingent on the size, number of bedrooms, number of bathrooms, and the number of garage bays. 

```{r}
#if no garage, recode from NA (and '') to 0; if 4+, recode to 4
la.dat$garage = as.numeric(ifelse(is.na(la.dat$garage),0,ifelse(la.dat$garage=='',0,ifelse(as.character(la.dat$garage)=='4+',4,as.character(la.dat$garage)))))

#recode pool: if "Y" --> 1, if " " --> 0
la.dat$pool = ifelse(la.dat$pool=='Y',1,0)
```

There are also several categorical factors that are expected to effect price, namely whether the property is a condo versus a house ('SFR' = single family residence). There are 39 observations that have no coded value for type (i.e, type = ''). These properties have a very different average value:

```{r}
tapply(la.dat$price,la.dat$type,mean)
```

I will recode them as "Alternative" (so that it comes first alphabetically and will thus automatically be the reference category).

```{r}
la.dat$type = ifelse(la.dat$type=='','Alternative',as.character(la.dat$type))
```

Other categorical variables are whether the residence has a pool and whether it has a spa. Finally, I am going to take the natural log of both price and square footage so that each variable is less skewed. This also makes conceptual sense, since we might expect that the marginal impact of square footage (or the marginal increase in price) is non-linear. For instance, a small 1000 sqft condo might cost a very different amount than a 2000 sqft condo, but 10,000 and 11,000 sqft mansions might be relatively more comparable in price. Taking the natural log means that we assume that the variable is linear in percentage terms. 

```{r}
par(mfrow=c(2,2))
hist(la.dat$price,breaks=100)
hist(la.dat$sqft,breaks=100)
hist(log(la.dat$price),breaks=100,main='log(price)')
hist(log(la.dat$sqft),breaks=100,main='log(price)')
```

```{r}
summary(la.mod1 <- lm(log(price) ~ log(sqft)*type + bed + bath + pool + garage,la.dat))
summary(la.mod2 <- lm(log(price) ~ log(sqft) + type + bed + bath + pool + garage,la.dat))
summary(la.mod3 <- lm(log(price) ~ log(sqft) + bed + bath + pool + garage,la.dat))
```

```{r}
library(stargazer);library(knitr)
stargazer(la.mod1,la.mod2,la.mod3,type='text',omit.stat=c("f", "rsq"),
          column.labels = c('M1',"M2","M3"),model.names = F,model.numbers = F)
```

The first model above regresses the log of price on log(sqft), the number of bedrooms, number of bathrooms, whether a house has a pool, and the number of garage bays. The model also includes an interaction term between sqft and home type (condo, house, or other), because I anticipate that the marginal impact of additional square footage on price is different for different classes of homes. The second model removes the interaction between type and square footage, and the third removes type entirely. 

```{r}
BIC(la.mod1,la.mod2,la.mod3)
```

The BIC scores indicate that while type of home appears to be significant from the regression output, it perhaps doesn't make much of a difference in practice. While model 3 has a slightly higher BIC score, considering that it has 4 fewer parameters it is probably a better model when factoring in parsimony. 


(@) Demonstrate the goodness-of-fit of your model (i.e, show that key assumptions appear to be met and that the model would seem to be a viable basis for inference). 

A quick plot of the distribution of residuals indicates that the residuals do appear to be relatively normally distributed. 
```{r}
par(mfrow=c(1,1))
hist(la.mod3$residuals,breaks=100)
```

The q-q plot looks pretty good as well:
```{r}
plot(la.mod3,2)
```

and when we plot the residuals versus fitted values, again there doesn't appear to be any major problems; the residuals seem to be randomly distributed around zero, and the variance appears constant across price levels. 
```{r}
plot(la.mod3,1)
```

```{r}
plot(la.mod3,5)
```

Finally, the plot of standardized residuals against leverage does not indicate any severe problems either. While there are some points that potentially have high leverage (e.g., 1294), there are no points that have high influence (i.e., every point has a relatively low Cook's distance, and the red line is relatively flat).

(@) Interpret your substantive findings.

```{r}
summary(la.mod3)
```

Since the dependent variable (price) is log-transformed, the linear coefficients can most easily be interpreted by exponentiating the given coefficient to produce a multiplicative effect. For instance, net of all other variables, each additional bedroom is predicted to decrease the price of a home by $exp(-0.105)$ = `r exp(-0.105)`. Thus, each bedroom is predicted to decrease the price by 10% (holding all else equal). This result might at first seem curious, but it is important to emphasize what these variables mean in this model. Since the model already controls for square footage, a 1 bedroom increase serves to compare the same sized house but with an additional room. It is likely that houses with fewer bedrooms but of similar size are fancier or have nicer features (or perhaps just really big rooms!). Pools and bathrooms are both associated with a predicted increase in price ($exp(0.28)$ = `r exp(0.28)` and $exp(0.047)$ = `r exp(0.047)` respectively). Finally, a 10% increase in square footage predicts a $1.10^{1.44}=$ `r round((1.10)^{1.44},2)` percent increase in price. 

(@) Discuss any potential shortcomings of this model and key future directions that you might take if you sought to better understand LA housing prices. 

Most likely the biggest shortcoming is that these data are not spatial; house values are driven not only by location but by surrounding homes. Variation can be very fine grained, since there might be small pockets of high- or low-value homes within a town. Thus, the most important next step would be to identify the location of homes that are sold and account for the fact that homes nearer to one another are likely more similar in price as well. 


### Problem 4.

(@) Again, using the LA housing price data, fit a model that estimates sqrt(price) solely as a function of sqrt(square footage) using maximum likelihood estimation (MLE) (hint: you'll need to use the `mle` function from the `stats4` package). Recall that in a linear regression, we assume that the residuals are normally distributed, so for MLE in this case we want a likelihood function that fits a normal distribution to the residuals:

```{r eval=FALSE}
#Note: you'll need to edit this slightly to make it work for your data
LL <- function(beta0, beta1, mu, sigma) {
    R = y - x * beta1 - beta0
    #
    R = suppressWarnings(dnorm(R, mu, sigma, log = TRUE))
    #
    -sum(R)
}
```

```{r}
LL <- function(beta0, beta1, mu, sigma) {
    R = sqrt(la.dat$price) - sqrt(la.dat$sqft) * beta1 - beta0
    #
    R = suppressWarnings(dnorm(R, mu, sigma, log = TRUE))
    #
    -sum(R)
}
library(stats4)
ml.est = mle(LL, start = list(beta0 = 10, beta1 = 10, sigma=1),
             fixed = list(mu = 0),
             nobs = length(la.dat$price))

summary(ml.est)
```

(@) Try several different starting parameter values: How consistent are your results? Do your results typically match up with results from a simple linear regression? What do you think accounts for your results?

```{r}
summary(lm(sqrt(price)~sqrt(sqft),data = la.dat))
```

If you use decent starting values, the results should be pretty close. In my case, a one-unit increase in $SqFt^{1/2}$ is predicted to increase the square root of the house price by 37.78, while for the OLS model the same coefficient is 37.91. The primary reason for the difference is that the MLE approach models the variance (notice that the MLE function takes an starting input for sigma) and provides an estimate for sigma. Also, the better the model fit the better you'll be able to estimate things. For instance, if you don't transform sqft and price, the model performs more poorly.  

(@) Perform the same analysis with but with the addition of a variable for number of bathrooms (i.e., $price ~ size + bathrooms$).

```{r}
LL2 <- function(beta0, beta1, beta2, mu, sigma) {
    R = sqrt(la.dat$price) - 
      la.dat$bath * beta2 - sqrt(la.dat$sqft) * beta1 - beta0
    #
    R = suppressWarnings(dnorm(R, mu, sigma, log = TRUE))
    #
    -sum(R)
}
library(stats4)
ml.est2 = mle(LL2, start = list(beta0 = 10, beta1 = 10, beta2 = 10,sigma=1),
             fixed = list(mu = 0),
             nobs = length(la.dat$price))

summary(ml.est2)
```

(@) In NO MORE than 4-6 sentences, explain how maximum likelihood estimation works in the context of this problem (i.e, how might you briefly describe your modeling approach within the context of a journal article methods section?)

The general tactic in maximum likelihood estimation is to identify a value for an unknown parameter (e.g., $\theta$) that maximizes the probability of observing that data that are observed. In other words, given the data that are observed, what is the parameter that is most likely to have given rise to those data? For the model above, this means that we identify estimates for $\beta_0$, $\beta_1$, and $\beta_3$ that maximize the likelihood function (in this case, based upon the assumption that the error terms $\epsilon_i$ are normally distributed with mean 0). 



### Report your process

You're encouraged to reflect on what was hard/easy, problems you solved, helpful tutorials you read, etc. Give credit to your sources, whether it's a blog post, a fellow student, an online tutorial, etc.

### Rubric

Minus: Didn't tackle at least 3 tasks. Or didn't make companion graphs. Didn't interpret anything but left it all to the "reader". Or more than one technical problem that is relatively easy to fix. It's hard to find the report in our repo.

Check: Completed, but not fully accurate and/or readable. Requires a bit of detective work on my part to see what you did

Check plus: Hits all the elements. No obvious mistakes. Pleasant to read. No heroic detective work required. Solid.



#### The command below is helpful for debugging, please don't change it

```{r echo=FALSE}
sessionInfo()
```









